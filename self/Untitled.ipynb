{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] 系统找不到指定的文件。",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-174a6207bba6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msubprocess\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ls\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"../input\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf200\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[1;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[1;32m--> 356\u001b[1;33m                **kwargs).stdout\n\u001b[0m\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf200\\lib\\subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(input, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    421\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stdin'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPIPE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m             \u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf200\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[0;32m    727\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 729\u001b[1;33m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[0;32m    730\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m             \u001b[1;31m# Cleanup if the child failed starting.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf200\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1015\u001b[0m                                          \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m                                          \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcwd\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1017\u001b[1;33m                                          startupinfo)\n\u001b[0m\u001b[0;32m   1018\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1019\u001b[0m                 \u001b[1;31m# Child is launched. Close the parent's copy of those pipe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] 系统找不到指定的文件。"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import imageio\n",
    "\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Maximum\n",
    "from keras.layers import ZeroPadding2D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras import regularizers\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.transform import resize as imresize\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCHS = 30\n",
    "RANDOM_STATE = 11\n",
    "\n",
    "CLASS = {\n",
    "    'Black-grass': 0,\n",
    "    'Charlock': 1,\n",
    "    'Cleavers': 2,\n",
    "    'Common Chickweed': 3,\n",
    "    'Common wheat': 4,\n",
    "    'Fat Hen': 5,\n",
    "    'Loose Silky-bent': 6,\n",
    "    'Maize': 7,\n",
    "    'Scentless Mayweed': 8,\n",
    "    'Shepherds Purse': 9,\n",
    "    'Small-flowered Cranesbill': 10,\n",
    "    'Sugar beet': 11\n",
    "}\n",
    "\n",
    "INV_CLASS = {\n",
    "    0: 'Black-grass',\n",
    "    1: 'Charlock',\n",
    "    2: 'Cleavers',\n",
    "    3: 'Common Chickweed',\n",
    "    4: 'Common wheat',\n",
    "    5: 'Fat Hen',\n",
    "    6: 'Loose Silky-bent',\n",
    "    7: 'Maize',\n",
    "    8: 'Scentless Mayweed',\n",
    "    9: 'Shepherds Purse',\n",
    "    10: 'Small-flowered Cranesbill',\n",
    "    11: 'Sugar beet'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layers set\n",
    "def dense_set(inp_layer, n, activation, drop_rate=0.):\n",
    "    dp = Dropout(drop_rate)(inp_layer)\n",
    "    dns = Dense(n)(dp)\n",
    "    bn = BatchNormalization(axis=-1)(dns)\n",
    "    act = Activation(activation=activation)(bn)\n",
    "    return act\n",
    "\n",
    "# Conv. layers set\n",
    "def conv_layer(feature_batch, feature_map, kernel_size=(3, 3),strides=(1,1), zp_flag=False):\n",
    "    if zp_flag:\n",
    "        zp = ZeroPadding2D((1,1))(feature_batch)\n",
    "    else:\n",
    "        zp = feature_batch\n",
    "    conv = Conv2D(filters=feature_map, kernel_size=kernel_size, strides=strides)(zp)\n",
    "    bn = BatchNormalization(axis=3)(conv)\n",
    "    act = LeakyReLU(1/10)(bn)\n",
    "    return act\n",
    "\n",
    "# simple model \n",
    "def get_model():\n",
    "    inp_img = Input(shape=(51, 51, 3))\n",
    "\n",
    "    # 51\n",
    "    conv1 = conv_layer(inp_img, 64, zp_flag=False)\n",
    "    conv2 = conv_layer(conv1, 64, zp_flag=False)\n",
    "    mp1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(conv2)\n",
    "    # 23\n",
    "    conv3 = conv_layer(mp1, 128, zp_flag=False)\n",
    "    conv4 = conv_layer(conv3, 128, zp_flag=False)\n",
    "    mp2 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(conv4)\n",
    "    # 9\n",
    "    conv7 = conv_layer(mp2, 256, zp_flag=False)\n",
    "    conv8 = conv_layer(conv7, 256, zp_flag=False)\n",
    "    conv9 = conv_layer(conv8, 256, zp_flag=False)\n",
    "    mp3 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(conv9)\n",
    "    # 1\n",
    "    # dense layers\n",
    "    flt = Flatten()(mp3)\n",
    "    ds1 = dense_set(flt, 128, activation='tanh')\n",
    "    out = dense_set(ds1, 12, activation='softmax')\n",
    "\n",
    "    model = Model(inputs=inp_img, outputs=out)\n",
    "    \n",
    "    # The first 50 epochs are used by Adam opt.\n",
    "    # Then 30 epochs are used by SGD opt.\n",
    "    \n",
    "    #mypotim = Adam(lr=2 * 1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    mypotim = SGD(lr=1 * 1e-1, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                   optimizer=mypotim,\n",
    "                   metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(filepath, patience=5):\n",
    "    lr_reduce = ReduceLROnPlateau(monitor='val_acc', factor=0.1, epsilon=1e-5, patience=patience, verbose=1)\n",
    "    msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "    return [lr_reduce, msave]\n",
    "\n",
    "# I trained model about 12h on GTX 950.\n",
    "def train_model(img, target):\n",
    "    callbacks = get_callbacks(filepath='model_weight_SGD.hdf5', patience=6)\n",
    "    gmodel = get_model()\n",
    "    gmodel.load_weights(filepath='model_weight_Adam.hdf5')\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "                                                        img,\n",
    "                                                        target,\n",
    "                                                        shuffle=True,\n",
    "                                                        train_size=0.8,\n",
    "                                                        random_state=RANDOM_STATE\n",
    "                                                        )\n",
    "    gen = ImageDataGenerator(\n",
    "            rotation_range=360.,\n",
    "            width_shift_range=0.3,\n",
    "            height_shift_range=0.3,\n",
    "            zoom_range=0.3,\n",
    "            horizontal_flip=True,\n",
    "            vertical_flip=True\n",
    "    )\n",
    "    gmodel.fit_generator(gen.flow(x_train, y_train,batch_size=BATCH_SIZE),\n",
    "               steps_per_epoch=10*len(x_train)/BATCH_SIZE,\n",
    "               epochs=EPOCHS,\n",
    "               verbose=1,\n",
    "               shuffle=True,\n",
    "               validation_data=(x_valid, y_valid),\n",
    "               callbacks=callbacks)\n",
    "\n",
    "def test_model(img, label):\n",
    "    gmodel = get_model()\n",
    "    gmodel.load_weights(filepath='../input/plant-weight/model_weight_SGD.hdf5')\n",
    "    prob = gmodel.predict(img, verbose=1)\n",
    "    pred = prob.argmax(axis=-1)\n",
    "    sub = pd.DataFrame({\"file\": label,\n",
    "                         \"species\": [INV_CLASS[p] for p in pred]})\n",
    "    sub.to_csv(\"sub.csv\", index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize all image to 51x51 \n",
    "def img_reshape(img):\n",
    "    img = imresize(img, (51, 51, 3))\n",
    "    return img\n",
    "\n",
    "# get image tag\n",
    "def img_label(path):\n",
    "    return str(str(path.split('/')[-1]))\n",
    "\n",
    "# get plant class on image\n",
    "def img_class(path):\n",
    "    return str(path.split('/')[-2])\n",
    "\n",
    "# fill train and test dict\n",
    "def fill_dict(paths, some_dict):\n",
    "    text = ''\n",
    "    if 'train' in paths[0]:\n",
    "        text = 'Start fill train_dict'\n",
    "    elif 'test' in paths[0]:\n",
    "        text = 'Start fill test_dict'\n",
    "\n",
    "    for p in tqdm(paths, ascii=True, ncols=85, desc=text):\n",
    "        img = imageio.imread(p)\n",
    "        img = img_reshape(img)\n",
    "        some_dict['image'].append(img)\n",
    "        some_dict['label'].append(img_label(p))\n",
    "        if 'train' in paths[0]:\n",
    "            some_dict['class'].append(img_class(p))\n",
    "\n",
    "    return some_dict\n",
    "\n",
    "# read image from dir. and fill train and test dict\n",
    "def reader():\n",
    "    file_ext = []\n",
    "    train_path = []\n",
    "    test_path = []\n",
    "\n",
    "    for root, dirs, files in os.walk('../input'):\n",
    "        if dirs != []:\n",
    "            print('Root:\\n'+str(root))\n",
    "            print('Dirs:\\n'+str(dirs))\n",
    "        else:\n",
    "            for f in files:\n",
    "                ext = os.path.splitext(str(f))[1][1:]\n",
    "\n",
    "                if ext not in file_ext:\n",
    "                    file_ext.append(ext)\n",
    "\n",
    "                if 'train' in root:\n",
    "                    path = os.path.join(root, f)\n",
    "                    train_path.append(path)\n",
    "                elif 'test' in root:\n",
    "                    path = os.path.join(root, f)\n",
    "                    test_path.append(path)\n",
    "    train_dict = {\n",
    "        'image': [],\n",
    "        'label': [],\n",
    "        'class': []\n",
    "    }\n",
    "    test_dict = {\n",
    "        'image': [],\n",
    "        'label': []\n",
    "    }\n",
    "\n",
    "    #train_dict = fill_dict(train_path, train_dict)\n",
    "    test_dict = fill_dict(test_path, test_dict)\n",
    "    return train_dict, test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train_dict, test_dict = reader()\n",
    "    #X_train = np.array(train_dict['image'])\n",
    "    #y_train = to_categorical(np.array([CLASS[l] for l in train_dict['class']]))\n",
    "\n",
    "    X_test = np.array(test_dict['image'])\n",
    "    label = test_dict['label']\n",
    "    \n",
    "    # I do not recommend trying to train the model on a kaggle.\n",
    "    #train_model(X_train, y_train)\n",
    "    test_model(X_test, label)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
